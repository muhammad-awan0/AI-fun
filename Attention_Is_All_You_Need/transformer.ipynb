{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Language Text:\n",
      "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'Zwei Männer stehen am Herd und bereiten Essen zu.', 'Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'Ein Mann lächelt einen ausgestopften Löwen an.', 'Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'Jungen tanzen mitten in der Nacht auf Pfosten.', 'Eine Ballettklasse mit fünf Mädchen, die nacheinander springen.', 'Vier Typen, von denen drei Hüte tragen und einer nicht, springen oben in einem Treppenhaus.', 'Ein schwarzer Hund und ein gefleckter Hund kämpfen.', 'Ein Mann in einer neongrünen und orangefarbenen Uniform fährt auf einem grünen Traktor.', 'Mehrere Frauen warten in einer Stadt im Freien.', 'Eine Frau mit schwarzem Oberteil und Brille streut Puderzucker auf einem Gugelhupf.', 'Ein kleines Mädchen sitzt vor einem großen gemalten Regenbogen.', 'Ein Mann liegt auf der Bank, an die auch ein weißer Hund angebunden ist.', 'Fünf Personen sitzen mit Instrumenten im Kreis.', 'Eine Gruppe älterer Frauen spielt zusammen Klarinette von Notenblättern.', 'Ein großes Bauwerk ist kaputt gegangen und liegt auf einer Fahrbahn.', 'Eine große Menschenmenge steht außen vor dem Eingang einer Metrostation.', 'Ein Mann, der ein Tattoo auf seinem Rücken erhält.', 'Zwei Kinder sitzen auf einer kleinen Wippe im Sand.', 'Ein Mann, der eine reflektierende Weste und einen Schutzhelm trägt, hält eine Flagge in die Straße.', 'Eine Person in einem blauen Mantel steht auf einem belebten Gehweg und betrachtet ein Gemälde einer Straßenszene.', 'Ein Mann in grünen Hosen läuft die Straße entlang.', 'Das kleine Kind klettert an roten Seilen auf einem Spielplatz.', 'Du weißt, dass ich aussehe wie Justin Bieber.', 'Ein junger Mann in einer schwarz-gelben Jacke blickt etwas an und lächelt.', 'Ein Mann, der mit einer Tasse Kaffee an einem Urinal steht.', 'Fünf gehende Personen mit einem mehrfarbigen Himmel im Hintergrund.')\n",
      "Target Language Text:\n",
      "('Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.', 'A man in a blue shirt is standing on a ladder cleaning a window.', 'Two men are at the stove preparing food.', 'A man in green holds a guitar while the other man observes his shirt.', 'A man is smiling at a stuffed lion', 'A trendy girl talking on her cellphone while gliding slowly down the street.', 'A woman with a large purse is walking by a gate.', 'Boys dancing on poles in the middle of the night.', 'A ballet class of five girls jumping in sequence.', 'Four guys three wearing hats one not are jumping at the top of a staircase.', 'A black dog and a spotted dog are fighting', 'A man in a neon green and orange uniform is driving on a green tractor.', 'Several women wait outside in a city.', 'A lady in a black top with glasses is sprinkling powdered sugar on a bundt cake.', 'A little girl is sitting in front of a large painted rainbow.', 'A man lays on the bench to which a white dog is also tied.', 'Five people are sitting in a circle with instruments.', 'A bunch of elderly women play their clarinets together as they read off sheet music.', 'A large structure has broken and is laying in a roadway.', 'A large crowd of people stand outside in front of the entrance to a Metro station.', 'A man getting a tattoo on his back.', 'Two children sit on a small seesaw in the sand.', 'A man wearing a reflective vest and a hard hat holds a flag in the road', 'A person dressed in a blue coat is standing in on a busy sidewalk, studying painting of a street scene.', 'A man in green pants walking down the road.', 'The small child climbs on a red ropes on a playground.', 'You know i am looking like Justin Bieber.', 'A young man in a black and yellow jacket is gazing at something and smiling.', 'A man standing at a urinal with a coffee cup.', 'Five people walking with a multicolored sky in the background.')\n"
     ]
    }
   ],
   "source": [
    "for batch, (src,tgt) in enumerate(train_dataloader):\n",
    "    for src1,tgt1 in zip(enumerate(src),enumerate(tgt)):\n",
    "        print(\"Source Language Text:\")\n",
    "        print(src)\n",
    "        print(\"Target Language Text:\")\n",
    "        print(tgt)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, language: str)\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 50\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, k, q, v, mask):\n",
    "        B, T, C = q.shape\n",
    "        _, S, _ = k.shape  # Source sequence length (could be different from T)\n",
    "\n",
    "        k_proj = self.key(k)\n",
    "        q_proj = self.query(q)\n",
    "        v_proj = self.value(v)\n",
    "\n",
    "        scores = torch.matmul(q_proj, k_proj.transpose(-2, -1)) / (C ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        out = torch.matmul(weights, v_proj)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, n_embd // n_head, dropout) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, k, q, v, mask=None):\n",
    "        out = torch.cat([head(k, q, v, mask) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attention(x, x, x, mask)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, dropout, device):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src):\n",
    "        B, T = src.size()\n",
    "        pos = torch.arange(0, T, device=self.device).unsqueeze(0)\n",
    "        x = self.token_embedding(src) + self.position_embedding[:, :T, :]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        x = x + self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.cross_attention(enc_output, x, enc_output, src_mask)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, dropout, device):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.out = nn.Linear(n_embd, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, trg, enc_output, src_mask, tgt_mask):\n",
    "        B, T = trg.size()\n",
    "        pos = torch.arange(0, T, device=self.device).unsqueeze(0)\n",
    "        x = self.token_embedding(trg) + self.position_embedding[:, :T, :]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_output, src_mask, tgt_mask)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, src_pad_idx, trg_pad_idx, device, n_embd=256, n_head=8, n_layer=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, n_embd, n_head, n_layer, dropout, device)\n",
    "        self.decoder = Decoder(vocab_size, n_embd, n_head, n_layer, dropout, device)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(-2).unsqueeze(-1)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.size()\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).unsqueeze(0).unsqueeze(1)\n",
    "        return trg_mask & (trg.unsqueeze(-2) != self.trg_pad_idx).unsqueeze(-1).unsqueeze(-2)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
